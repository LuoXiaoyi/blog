---
layout:    post
title:     Linux CPU 使用率
category:  Linux
tags:  CPU 使用率
---
> 研究这个问题的背景是，偶然的机会我发现 top 出来的机器所有 CPU 的 sy 和 us 竟然比某个一个进程的总 CPU 的使用率要低的多。具体如下
<pre>
%Cpu0  :  0.7 us, 18.3 sy, 48.8 ni, 20.1 id,  0.0 wa,  0.0 hi, 12.1 si,  0.0 st
%Cpu1  :  0.7 us, 23.2 sy, 64.5 ni, 11.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  3881668 total,   426364 free,  2281960 used,  1173344 buff/cache
KiB Swap:  4063228 total,  4057184 free,     6044 used.  1079336 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4226 admin     25   5 3723292 766840  14200 S 169.7 19.8   5793:50 java
</pre>
从内容中我们可以清晰的发现，进程 4226 所占用的 CPU 使用率为 169%，而 Cpu0 和 Cpu1 的 us 与 sy 的综合加起来才 42.9%，远远低于进程的 CPU 使用率。第一眼看上去简直不敢相信自己的眼睛。带着如下问题，我们来一探究竟。
* 该如何去看这里的各个指标，即每一个指标到底是什么含义？
* top 计算的数据源是什么？如何产生的？
* top 又是如何计算的？

## CPU 指标介绍
我们首先来看看每个 CPU 指标到底都是什么意思？
### 针对 CPU
<pre>
%Cpu0  :  0.7 us, 18.3 sy, 48.8 ni, 20.1 id,  0.0 wa,  0.0 hi, 12.1 si,  0.0 st
</pre>
以上是一个 CPU core 的相关指标的数据百分比，如 ```18.3 sy``` 则表示当前 Cpu0 运行在内核态的时间占整个 CPU 时间的 ```18.3%```。具体详细信息如下，可以参见 ```man top```
* us, user    : time running un-niced user processes，这里需要特别注意的一点是 ```un-niced``` 这个定语，指的是该指标是 ```非nice``` 的所有进程运行在用户态下的 CPU 所占的百分比；这与 ```ni``` 是互斥的，但是二者相加所得之和则表示当前 CPU 所有在用户态下所运行的时间的百分比。
* sy, system  : time running kernel processes
* ni, nice    : time running niced user processes，这里指的是 CPU 运行在设置过进程 ```nice``` 值的进程时所消耗的 CPU 的百分比，同样属于用户态。至于什么是 nice 过的进程将在后面细讲。
* id, idle    : time spent in the kernel idle handler
* wa, IO-wait : time waiting for I/O completion
* hi : time spent servicing hardware interrupts
* si : time spent servicing software interrupts
* st : time stolen from this vm by the hypervisor

### 针对进程
在 top 命令下，我们可以很清晰的看到，进程 ```4226``` 消耗的 CPU 使用率为 ```169.7%```，在多核环境下，这意味着该进程 CPU 的总消耗接近了整体 CPU 的 ```85%``` (进程总的 CPU 使用率 169.7% / CPU核心数，在这个实例中，CPU 核心数为 2)。这个进程的 CPU 使用率，是由进程的 ```用户态``` 和 ```核心态``` 二者相加的结果。

### 总结
通过上面的分析可以发现，从 CPU 维度分析 CPU 使用率与从进程维度分析 CPU 使用率时，所区分的维度是不一样。如果要将 CPU 维度的使用率与进程的 CPU 使用率对应起来，根据 ```核心态``` 是指 CPU 运行在系统调用中所消耗的时间，而 ```用户态``` 是 CPU 运行在用户代码所消耗的时间这一定义来看的话，有如下具体对应
* 进程用户态 = 所有 CPU 的（user + nice）之和
* 进程的核心态 = 所有 CPU 的（system + wa + hi + si + st）之和

这样，文章开头抛出来的 CPU0 + CPU1 总的 sy + usr < 进程 CPU 使用率的问题就得到的解答。即不能这么比，正确的比较应该是：
CPU0 总的 CPU 消耗
    用户态：0.7 + 48.8 = 49.5
    核心态：18.3 + 12.1 = 30.4

CPU1 总的 CPU 消耗
    用户态：0.7 + 64.5 = 65.2
    核心态：23.2

CPU 总的用户态 + 核心态为：（49.5 + 65.2）+ （30.4 + 23.2）= 168.3%，而这个值和进程 ```4226``` 的 169.7% 是很接近的。至于这里的一点点误差由于在计算时，读取相关 proc 文件的顺序与时间的先后关系与不一致造成的，属于正常情况。

## Top 计算的数据源以及其产生的原理

## Top 是如何利用数据源进行计算

## Nice 计算原理
ps -eo nice,pid,args | awk '$1 != 0 && $1 != "-" {print $0}' 
ps -eo nice,pid,args | grep '^\s*[1-9]'
ps -ao "%p%y%x%c%n"   


https://www.thegeekdiary.com/unix-linux-how-to-change-the-niceness-priority-of-a-process/
https://askubuntu.com/questions/399357/what-does-the-nice-value-mean-in-cpu-utilization-statistics
https://www.howtoforge.com/linux-nice-command/
https://www.halolinux.us/kernel-architecture/elements-in-the-task-structure.html


coreutils:  nice.c
	|
	|----> linux kernel : sys.c
							|
							|--> SYSCALL_DEFINE3(setpriority)
											|
											|--> set_one_prio
													|
													|--> set_one_prio_perm
													|--> set_user_nice  (sched/core.c)
															|
															|--> p->static_prio = NICE_TO_PRIO(nice) (sched/sched.h)
																						|
																						|--> NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)

linux kernel: (sched/cputime.c)
		|----> account_system_time (更新任务的 kernel system time)
					|---> account_guest_time
								|---> p->utime += cputime;
								|---> p->gtime += cputime;
					|---> __account_system_time
								|---> p->stime += cputime; (更新进程的 stime)

		|----> account_guest_time
					|
					|---> kcpustat_this_cpu (&__get_cpu_var(kernel_cpustat))
					|---> TASK_NICE(p) > 0
								|
								|--> cpustat[CPUTIME_NICE] += (__force u64) cputime;
								|--> cpustat[CPUTIME_GUEST_NICE] += (__force u64) cputime;

		|----> account_user_time
					|
					|--> p->utime += cputime; (更新进程的 utime)
					|--> index = (TASK_NICE(p) > 0) ? CPUTIME_NICE : CPUTIME_USER;  (sched/sched.h)
										|
										|--> PRIO_TO_NICE((p)->static_prio)
													|
													|--> PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
					|
					|--> task_group_account_field(p, index, (__force u64) cputime); 
									|
									|--> __get_cpu_var(kernel_cpustat).cpustat[index] += tmp;	(get current cpu)
															|
															|--> struct kernel_cpustat {  (linux/kernel_stat.h)
																	u64 cpustat[NR_STATS];
																 };					|
																 					|---> enum cpu_usage_stat {
																							CPUTIME_USER,
																							CPUTIME_NICE,
																							CPUTIME_SYSTEM,
																							CPUTIME_SOFTIRQ,
																							CPUTIME_IRQ,
																							CPUTIME_IDLE,
																							CPUTIME_IOWAIT,
																							CPUTIME_STEAL,
																							CPUTIME_GUEST,
																							CPUTIME_GUEST_NICE,
																							NR_STATS,
																						}; 
					|--> cpuacct_account_field(p, index, tmp);  (sched/cpuacct.c it's the caller that updates the account of the root cgroup.)


linux kernel: (proc/stat.c)
		|
		|-----> show_stat
					|
					|----> nice += kcpustat_cpu(i).cpustat[CPUTIME_NICE];  (total machine cpu nice)
					|----> nice = kcpustat_cpu(i).cpustat[CPUTIME_NICE];   (per cpu nice)	


linux kernel: (fs/proc/array.c)
	|
	|----> do_task_stat (这个函数会对 pid 和 task 做不同的操作，如果读取的 /proc/$pid/stat，则会把 /proc/$pid/task/ 所有的 task 的 utime、stime 加起来
				|		，而如果只是 /proc/$pid/task/$taskId/stat 那就只会读取某一个 stat 文件的 utime、stime)
				|---> utime
						|--> task.utime(内核会做一些维护，可以简单这么认为，会加上 task.sig->utime)
								|--> thread_group_cputime_adjusted 
										660	 	task_cputimecollected CPU time counts
										662	 	Accumulate raw cputime values of dead tasks(sig->[us]time) and live tasks(sum on group iteration) belonging to@tsk's group.
										663	 	Adjust tick based cputime random precision against scheduler runtime accounting.
											|
											|--> thread_group_cputime方法in cputime.c
				|---> cutime
						|--> signal_struct.cutime
				|---> stime
						|--> task.stime(内核会做一些维护，可以简单这么认为，会加上 task.sig->stime)
								|--> thread_group_cputime_adjusted 
											|--> thread_group_cputime方法in cputime.c
				|---> cstime
						|--> signal_struct.cstime
				|---> guest_time
						|--> task.gtime
				|---> cguest_time	
						|--> signal_struct->cgtime											


linux kernel: (kernel/exit.c)
	|
	|----> __exit_signal(struct task_struct *tsk)
				|--> 


process:
	|--->utime  	  (14) Amount of time that this process has been  scheduled
		                  in user  mode,  measured  in  clock  ticks  (divide  by
		                  sysconf(_SC_CLK_TCK)).    This   includes   guest   time,
		                  guest_time (time spent running a virtual CPU, see below),
		                  so that applications that are not aware of the guest time
		                  field do not lose that time from their calculations.
	|--->stime  	  (15)  Amount of time that this process has been scheduled
		                  in kernel  mode,  measured  in  clock  ticks  (divide  by
		                  sysconf(_SC_CLK_TCK)).
	|--->cstime 	  (16)  Amount of time that this process's waited-for chil‐
		                  dren have been scheduled in user mode, measured in  clock
		                  ticks   (divide   by  sysconf(_SC_CLK_TCK)).   (See  also
		                  times(2).)  This includes guest time,  cguest_time  (time
		                  spent running a virtual CPU, see below).
	|--->cutime 	  (17)  Amount of time that this process's waited-for chil‐
	                  	  dren have been scheduled  in  kernel  mode,  measured  in
	                  	  clock ticks (divide by sysconf(_SC_CLK_TCK)).
    |--->guest_time   (43) Guest time of the process (time spent running a vir‐
                          tual CPU for a guest operating system), measured in clock
                          ticks (divide by sysconf(_SC_CLK_TCK)).
    |--->cguest_time  (44) Guest time of the process's  children,  measured  in
                          clock ticks (divide by sysconf(_SC_CLK_TCK)).

进程的 CPU 使用：
	task.utime + task.cutime + task.stime + task.cstime

CPU 的用户态+系统态：
	cpu.user + cpu.system


用户态 CPU
	|---> 进程：task.utime = user + guest + sig->utime(thread_group_cputime方法in cputime.c)
	|---> CPU: cpu.user(include guest) 	+  cpu.nice(include guest_nice)

System CPU
	|---> 进程：task.stime + sig->stime(thread_group_cputime_adjusted -> thread_group_cputime方法in cputime.c)
	|---> CPU: cpu.irq + cpu.softirq + cpu.system (这个3 个值相加会略大于进程的 task.stime 之和)
					

update_process_times
		|--> account_process_tick


OS 是时更新 proc 文件系统的？/proc/stat 先更新  还是 /proc/$pid/stat 先更新？二者更新的频率是多少？
https://unix.stackexchange.com/questions/74713/how-frequently-is-the-proc-file-system-updated-on-linux
https://unix.stackexchange.com/questions/121702/what-happens-when-i-run-the-command-cat-proc-cpuinfo


top.c 
// 计算进程的总的 CPU 消耗
hist_new[Frame_maxtask].tics = tics = (this->utime + this->stime);

// tics 和之前的 tics 做减法
if(ptr) tics -= ptr->tics;
this->pcpu = tics;

Hertz =  100 /* normal Linux */
// 计算两个时间段所有 CPU 的耗时，用作 CPU 使用率计算的分母
et = (timev.tv_sec - oldtimev.tv_sec)
         + (float)(timev.tv_usec - oldtimev.tv_usec) / 1000000.0;

Frame_tscale = 100.0f / ((float)Hertz * (float)et * (Rc.mode_irixps ? 1 : Cpu_tot));


内核插桩

----------------------------- proc fs init-----------------------------
start_kernel
	|--> proc_root_init
			|--> register_filesystem
						|--> proc_mount
								|--> proc_fill_super
										|--> proc_get_inode(struct super_block *sb, struct proc_dir_entry *de)
												|--> inode->i_fop = de->proc_fops;  (proc_root.proc_fops)
												|--> inode->i_op = de->proc_iops;	(proc_root.proc_iops)

proc_root
	/*
	 * This is the root "inode" in the /proc tree..
	 */
	struct proc_dir_entry proc_root = {
		.low_ino	= PROC_ROOT_INO, 
		.namelen	= 5, 
		.mode		= S_IFDIR | S_IRUGO | S_IXUGO, 
		.nlink		= 2, 
		.count		= ATOMIC_INIT(1),
		.proc_iops	= &proc_root_inode_operations, 
		.proc_fops	= &proc_root_operations,
		.parent		= &proc_root,
		.name		= "/proc",
	};


proc_root_inode_operations
		|--> proc_root_lookup
					|--> proc_pid_lookup
								|--> proc_pid_instantiate
											|--> inode->i_op = &proc_tgid_base_inode_operations;
											|--> inode->i_fop = &proc_tgid_base_operations;

static const struct inode_operations proc_tgid_base_inode_operations = {
	.lookup		= proc_tgid_base_lookup,
	.getattr	= pid_getattr,
	.setattr	= proc_setattr,
	.permission	= proc_pid_permission,
};

proc_tgid_base_lookup
		|---> proc_pident_lookup(tgid_base_stuff)
										|---> 这个是一个函数数组，里面定义了各个 pid 下的子文件的动作函数
										|---> DIR("task",       S_IRUGO|S_IXUGO, proc_task_inode_operations, proc_task_operations)  
												|---> 这是其中比较特殊的一项，定义了对 task 目录下所有子任务的操作

static const struct file_operations proc_tgid_base_operations = {
	.read		= generic_read_dir,
	.readdir	= proc_tgid_base_readdir,
	.llseek		= default_llseek,
};

----------------------------  tasks ----------------------------
base.c
static const struct inode_operations proc_task_inode_operations = {
	.lookup		= proc_task_lookup,
	.getattr	= proc_task_getattr,
	.setattr	= proc_setattr,
	.permission	= proc_pid_permission,
};

static const struct file_operations proc_task_operations = {
	.read		= generic_read_dir,
	.readdir	= proc_task_readdir,
	.llseek		= default_llseek,
};

	proc_task_lookup
		|---> proc_task_instantiate
					|--> inode->i_op = &proc_tid_base_inode_operations;
					|--> inode->i_fop = &proc_tid_base_operations;

	static const struct file_operations proc_tid_base_operations = {
		.read		= generic_read_dir,
		.readdir	= proc_tid_base_readdir,
		.llseek		= default_llseek,
	};

	static const struct inode_operations proc_tid_base_inode_operations = {
	.lookup		= proc_tid_base_lookup,
	.getattr	= pid_getattr,
	.setattr	= proc_setattr,
	};

	|---> proc_tid_base_readdir
		|---> tid_base_stuff[]
				|--> 这个是一个函数数组，里面定义了各个 task 目录下的子文件的动作函数
				如：ONE("stat",      S_IRUGO, proc_tid_stat),
												|  
												|---> do_task_stat(m, ns, pid, task, 0);   // in array.c 
	|---> proc_tid_base_lookup
		|---> tid_base_stuff[]